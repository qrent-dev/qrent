# -*- coding: utf-8 -*-
import os
import json
from dotenv import load_dotenv, find_dotenv
from openai import OpenAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings

# å¯¼å…¥functionæ¨¡å—
try:
    from function import AVAILABLE_FUNCTIONS
except ImportError:
    AVAILABLE_FUNCTIONS = {}

# Load environment variables
dotenv_path = find_dotenv()
if dotenv_path:
    load_dotenv(dotenv_path)

API_KEY = os.getenv("API_KEY_POINT")

# Configuration
INDEX_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "database", "faiss_index")
EMBEDDING_REPO = "qwen/Qwen3-Embedding-0.6B"
DASHSCOPE_BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
QWEN_MODEL = "qwen-vl-max-latest"


class QrentAgent:
    def __init__(self):
        self.vector_store = None
        self.embed_model = None
        self.history = []
        self.questionnaire_data = None
        self.inquiry_agent_history = None
        self.inquiry_updated_requirements = None
        self._initialize()
    
    def _initialize(self):
        """Initialize Agent"""
        try:
            # Initialize embedding model
            self.embed_model = HuggingFaceEmbeddings(
                model_name=EMBEDDING_REPO,
                model_kwargs={"device": "cpu"}
            )
            
            # Load vector store
            self.vector_store = self._load_vector_store()
            
            print("QrentAgent initialized successfully")
        except Exception as e:
            print(f"Error initializing QrentAgent: {e}")
            raise
    
    def _load_vector_store(self) -> FAISS:
        """Load vector store"""
        try:
            if not self.embed_model:
                raise ValueError("Embedding model not initialized")
                
            # Check if local index exists
            if not os.path.isdir(INDEX_DIR) or not os.listdir(INDEX_DIR):
                raise ValueError(f"Vector store not found at {INDEX_DIR}. Please build the vector store first.")
            
            # Load existing vector store
            return FAISS.load_local(
                folder_path=INDEX_DIR,
                embeddings=self.embed_model,
                allow_dangerous_deserialization=True
            )
        except Exception as e:
            print(f"Error loading vector store: {e}")
            raise
    
    def update_context(self, questionnaire_data=None, inquiry_agent_history=None, inquiry_updated_requirements=None):
        """Update context information including questionnaire data, inquiry agent history, and updated requirements"""
        if questionnaire_data is not None:
            self.questionnaire_data = questionnaire_data
        if inquiry_agent_history is not None:
            self.inquiry_agent_history = inquiry_agent_history
        if inquiry_updated_requirements is not None:
            self.inquiry_updated_requirements = inquiry_updated_requirements
    
    def retrieve_vector_context(self, query: str, top_k: int = 5):
        """Retrieve relevant context from vector store"""
        if not self.vector_store:
            raise ValueError("Vector store not initialized")
        
        retriever = self.vector_store.as_retriever(search_kwargs={"k": top_k})
        docs = retriever.invoke(query)
        context = "\n---\n".join([d.page_content for d in docs])
        ids = [d.metadata.get("id") for d in docs if d.metadata.get("id")]
        return context, ids
    
    def call_qwen_via_dashscope(self, messages: list, use_functions: bool = True) -> dict:
        """Call Qwen model to generate response with optional function calling"""
        if not API_KEY:
            raise ValueError("API_KEY_POINT not set in environment variables")
        
        client = OpenAI(api_key=API_KEY, base_url=DASHSCOPE_BASE_URL)
        
        # è¿‡æ»¤å’Œè½¬æ¢æ¶ˆæ¯è§’è‰²ï¼Œç¡®ä¿APIå…¼å®¹æ€§
        filtered_messages = []
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            
            # å°†éæ ‡å‡†è§’è‰²æ˜ å°„ä¸ºassistant
            if role == "inquiry_assistant":
                role = "assistant"
            
            # åªä¿ç•™APIæ”¯æŒçš„è§’è‰²
            if role in ["system", "assistant", "user", "tool", "function"]:
                filtered_messages.append({"role": role, "content": content})
        
        try:
            # å‡†å¤‡å‡½æ•°å®šä¹‰
            functions = None
            if use_functions and AVAILABLE_FUNCTIONS:
                functions = []
                for func_name, func_config in AVAILABLE_FUNCTIONS.items():
                    functions.append({
                        "type": "function",
                        "function": {
                            "name": func_name,
                            "description": func_config["description"],
                            "parameters": func_config["parameters"]
                        }
                    })
                print(f"Debug: Using {len(functions)} functions: {[f['function']['name'] for f in functions]}")
            else:
                print("Debug: No functions available or function calling disabled")
            
            # è°ƒç”¨API
            if functions:
                completion = client.chat.completions.create(
                    model=QWEN_MODEL,
                    messages=filtered_messages,
                    tools=functions,
                    tool_choice="auto"
                )
            else:
                completion = client.chat.completions.create(
                    model=QWEN_MODEL,
                    messages=filtered_messages
                )
            
            response_message = completion.choices[0].message
            
            # å¤„ç†å‡½æ•°è°ƒç”¨
            if hasattr(response_message, 'tool_calls') and response_message.tool_calls:
                tool_calls = response_message.tool_calls
                function_results = []
                
                for tool_call in tool_calls:
                    function_name = tool_call.function.name
                    
                    try:
                        # æ·»åŠ JSONè§£æçš„é”™è¯¯å¤„ç†
                        arguments_str = tool_call.function.arguments
                        print(f"Debug: Parsing function arguments: {arguments_str}")
                        
                        # å°è¯•æ¸…ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
                        arguments_str = arguments_str.strip()
                        
                        # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                        if arguments_str.startswith('```json'):
                            arguments_str = arguments_str.replace('```json', '').replace('```', '').strip()
                        elif arguments_str.startswith('```'):
                            arguments_str = arguments_str.replace('```', '').strip()
                        
                        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„JSONå¯¹è±¡
                        # å¦‚æœæœ‰é¢å¤–çš„æ•°æ®ï¼Œåªå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                        try:
                            # é¦–å…ˆå°è¯•ç›´æ¥è§£æ
                            function_args = json.loads(arguments_str)
                        except json.JSONDecodeError:
                            # å¦‚æœå¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                            decoder = json.JSONDecoder()
                            try:
                                function_args, idx = decoder.raw_decode(arguments_str)
                                print(f"Debug: Found JSON object ending at position {idx}, total length: {len(arguments_str)}")
                                if idx < len(arguments_str.strip()):
                                    print(f"Debug: Extra data after JSON: '{arguments_str[idx:].strip()}'")
                            except json.JSONDecodeError:
                                # æœ€åçš„å°è¯•ï¼šæŸ¥æ‰¾{}æ‹¬èµ·æ¥çš„å†…å®¹
                                import re
                                json_match = re.search(r'\{.*\}', arguments_str, re.DOTALL)
                                if json_match:
                                    json_part = json_match.group(0)
                                    function_args = json.loads(json_part)
                                    print(f"Debug: Extracted JSON from regex: {json_part}")
                                else:
                                    raise
                        
                    except json.JSONDecodeError as e:
                        print(f"JSON decode error for function {function_name}: {e}")
                        print(f"Raw arguments: {tool_call.function.arguments}")
                        print(f"Error position: line {e.lineno}, column {e.colno}")
                        function_results.append({
                            "name": function_name,
                            "error": f"å‚æ•°è§£æé”™è¯¯: {e}"
                        })
                        continue
                    except Exception as e:
                        print(f"Unexpected error parsing function arguments: {e}")
                        function_results.append({
                            "name": function_name,
                            "error": f"å‚æ•°è§£æé”™è¯¯: {e}"
                        })
                        continue
                    
                    if function_name in AVAILABLE_FUNCTIONS:
                        func = AVAILABLE_FUNCTIONS[function_name]["function"]
                        try:
                            result = func(**function_args)
                            function_results.append({
                                "name": function_name,
                                "result": result
                            })
                        except Exception as e:
                            function_results.append({
                                "name": function_name,
                                "error": f"å‡½æ•°è°ƒç”¨é”™è¯¯: {e}"
                            })
                    else:
                        function_results.append({
                            "name": function_name,
                            "error": f"æœªçŸ¥å‡½æ•°: {function_name}"
                        })
                
                return {
                    "type": "function_call",
                    "function_results": function_results,
                    "original_message": response_message.content or ""
                }
            else:
                # æ™®é€šæ–‡æœ¬å›å¤
                return {
                    "type": "text",
                    "content": response_message.content or ""
                }
                
        except json.JSONDecodeError as e:
            print(f"JSON decode error in Qwen API response: {e}")
            print(f"Error position: line {e.lineno}, column {e.colno}")
            print(f"Error character: {e.pos}")
            # è¿”å›ä¸€ä¸ªå®‰å…¨çš„é”™è¯¯å“åº”è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return {
                "type": "text",
                "content": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†æ ¼å¼é”™è¯¯ã€‚è¯·ç¨åé‡è¯•æˆ–ç®€åŒ–æ‚¨çš„é—®é¢˜ã€‚"
            }
        except Exception as e:
            print(f"Error calling Qwen API: {e}")
            print(f"Error type: {type(e)}")
            # è¿”å›ä¸€ä¸ªå®‰å…¨çš„é”™è¯¯å“åº”
            return {
                "type": "text", 
                "content": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é”™è¯¯ï¼š{str(e)}"
            }
    
    def generate_prompt(self, query: str, vector_context: str) -> str:
        """Generate prompt for AI model"""
        # æ£€æµ‹ç”¨æˆ·æŸ¥è¯¢è¯­è¨€å¹¶è®¾ç½®ç›¸åº”çš„æç¤ºè¯
        def contains_chinese(text):
            """æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦"""
            for char in text:
                if '\u4e00' <= char <= '\u9fff':
                    return True
            return False
        
        is_chinese = contains_chinese(query)
        
        # æ ¼å¼åŒ–é—®å·æ•°æ®å’Œæ›´æ–°åçš„éœ€æ±‚æ•°æ®
        questionnaire_context = ""
        if self.questionnaire_data:
            questionnaire_context = "# ç”¨æˆ·åŸå§‹é—®å·ä¿¡æ¯\n\n" if is_chinese else "# User Original Questionnaire Information\n\n"
            for key, value in self.questionnaire_data.items():
                if value:
                    questionnaire_context += f"- {key}: {value}\n"
            questionnaire_context += "\n"
        
        # æ·»åŠ æ›´æ–°åçš„éœ€æ±‚æ•°æ®
        updated_requirements_context = ""
        if self.inquiry_updated_requirements:
            updated_requirements_context = "# æ™ºèƒ½åˆ†æå¸ˆæ›´æ–°åçš„éœ€æ±‚ä¿¡æ¯\n\n" if is_chinese else "# Updated Requirements from Intelligent Analyst\n\n"
            updated_requirements_context += "ç»è¿‡ä¸“ä¸šéœ€æ±‚åˆ†æå¸ˆåˆ†æå’Œè¿½é—®åï¼Œç”¨æˆ·çš„æœ€æ–°éœ€æ±‚å¦‚ä¸‹ï¼š\n\n" if is_chinese else "After professional requirement analysis and follow-up questions, the user's latest requirements are:\n\n"
            
            for key, value in self.inquiry_updated_requirements.items():
                if value is not None:
                    updated_requirements_context += f"- {key}: {value}\n"
            updated_requirements_context += "\n**æ³¨æ„ï¼šè¿™äº›æ˜¯ç»è¿‡ä¸“ä¸šè¯„ä¼°å’Œç”¨æˆ·ç¡®è®¤çš„æœ€æ–°éœ€æ±‚ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›æ•°æ®è¿›è¡Œæˆ¿æºæ¨èã€‚**\n\n" if is_chinese else "\n**Note: These are the latest requirement information after professional assessment and user confirmation. Please prioritize these data for property recommendations.**\n\n"
        
        # æ ¼å¼åŒ–inquiry_agentå†å²
        inquiry_context = ""
        if self.inquiry_agent_history:
            inquiry_context = "# éœ€æ±‚åˆ†æå¸ˆè¯„ä¼°å†å²\n\n" if is_chinese else "# Requirement Analyst Assessment History\n\n"
            for i, (role, content) in enumerate(self.inquiry_agent_history[-4:]):  # åªå–æœ€è¿‘2è½®å¯¹è¯
                role_name = "ç”¨æˆ·" if role == "user" else "éœ€æ±‚åˆ†æå¸ˆ" if is_chinese else ("User" if role == "user" else "Requirement Analyst")
                inquiry_context += f"**{role_name}:** {content[:300]}{'...' if len(content) > 300 else ''}\n\n"
        
        if is_chinese:
            # ä¸­æ–‡æç¤ºè¯
            prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šçš„ç§Ÿæˆ¿ä¸­ä»‹åŠ©æ‰‹ï¼Œæ ¹æ®å…¬å¸çŸ¥è¯†åº“ä¿¡æ¯ä¸ºç”¨æˆ·æä¾›ç§Ÿæˆ¿å»ºè®®ã€‚è¯·ä»”ç»†é˜…è¯»çŸ¥è¯†åº“å†…å®¹ã€ç”¨æˆ·é—®å·ä¿¡æ¯ã€éœ€æ±‚åˆ†æå¸ˆçš„è¯„ä¼°ç»“æœã€å¯¹è¯å†å²å’Œç”¨æˆ·å½“å‰é—®é¢˜ï¼Œç„¶åæä¾›ä¸“ä¸šçš„ç§Ÿæˆ¿å»ºè®®ã€‚

# çŸ¥è¯†åº“ä¸Šä¸‹æ–‡

{vector_context}

{questionnaire_context}

{updated_requirements_context}

{inquiry_context}

# å¯¹è¯å†å²

{self.history}

# ç”¨æˆ·å½“å‰é—®é¢˜

{query}

# è¦æ±‚
- è¯·ç”¨ä¸­æ–‡å›ç­”
- ç»“åˆç”¨æˆ·é—®å·ä¿¡æ¯å’Œéœ€æ±‚åˆ†æå¸ˆçš„è¯„ä¼°ç»“æœæä¾›å»ºè®®
- æ ¹æ®çŸ¥è¯†åº“ä¿¡æ¯æä¾›å…·ä½“çš„ç§Ÿæˆ¿å»ºè®®
- åŒ…å«å…·ä½“çš„åœ°åŒºã€ä»·æ ¼ã€æˆ¿å‹ç­‰ä¿¡æ¯
- å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯¢é—®ç”¨æˆ·æ›´å¤šç»†èŠ‚
- ä¿æŒä¸“ä¸šã€å‹å¥½çš„è¯­è°ƒ
- å……åˆ†è€ƒè™‘ç”¨æˆ·çš„é¢„ç®—ã€æˆ¿å‹åå¥½å’Œç‰¹æ®Šéœ€æ±‚
"""
        else:
            # è‹±æ–‡æç¤ºè¯
            prompt = f"""
You are a professional rental assistant. Based on the knowledge base information, provide rental suggestions for users.

# Knowledge Base Context

{vector_context}

{questionnaire_context}

{updated_requirements_context}

{inquiry_context}

# Conversation History

{self.history}

# Current User Query

{query}

# Requirements
- Please respond in user's language
- Provide recommendations based on user questionnaire and requirement analyst assessment
- Provide specific rental recommendations based on the knowledge base
- Include specific areas, prices, property types, etc.
- If information is insufficient, ask for more details from the user
- Maintain a professional and friendly tone
- Consider user's budget, room type preferences, and special requirements
"""
        
        return prompt
    
    def process_query(self, query: str, top_k: int = 5, use_functions: bool = True) -> dict:
        """Process user query and return result"""
        try:
            # Vector retrieval
            vector_context, ids = self.retrieve_vector_context(query, top_k)
            
            # Build message list
            messages = [
                {"role": "system", "content": "You are a helpful rental assistant with access to real estate database functions."}
            ]
            
            # Add history (filter and convert roles for API compatibility)
            for role, content in self.history:
                # å°†éæ ‡å‡†è§’è‰²æ˜ å°„ä¸ºassistant
                if role == "inquiry_assistant":
                    role = "assistant"
                
                # åªä¿ç•™APIæ”¯æŒçš„è§’è‰²
                if role in ["system", "assistant", "user", "tool", "function"]:
                    messages.append({"role": role, "content": content})
            
            # Generate prompt
            prompt = self.generate_prompt(query, vector_context)
            messages.append({"role": "user", "content": prompt})
            
            # Call AI model
            response = self.call_qwen_via_dashscope(messages, use_functions)
            
            # å¤„ç†ä¸åŒç±»å‹çš„å“åº”
            final_answer = ""
            function_results = []
            
            if response["type"] == "function_call":
                # å¤„ç†å‡½æ•°è°ƒç”¨ç»“æœ
                function_results = response["function_results"]
                
                # æ„å»ºåŒ…å«å‡½æ•°ç»“æœçš„æ–°æ¶ˆæ¯
                function_summary = "åŸºäºæ•°æ®åº“æŸ¥è¯¢ç»“æœï¼š\n\n"
                for func_result in function_results:
                    if "error" in func_result:
                        function_summary += f"âŒ {func_result['name']}: {func_result['error']}\n"
                    else:
                        result = func_result["result"]
                        if result.get("success"):
                            function_summary += f"âœ… {func_result['name']} æŸ¥è¯¢æˆåŠŸ\n"
                            if "analysis_results" in result:
                                # æ ¼å¼åŒ–åŒºåŸŸåˆ†æç»“æœ
                                for area, analysis in result["analysis_results"].items():
                                    function_summary += f"\nğŸ“ {area.upper()}åŒºåŸŸ:\n"
                                    function_summary += f"  æ€»æˆ¿æº: {analysis['total_properties']}å¥—\n"
                                    for room_type, stats in analysis['room_types'].items():
                                        function_summary += f"  {room_type}: {stats['count']}å¥—, å¹³å‡ç§Ÿé‡‘{stats['avg_price']}AUD/å‘¨\n"
                            elif "properties" in result:
                                # æ ¼å¼åŒ–æˆ¿æºæœç´¢ç»“æœ  
                                function_summary += f"æ‰¾åˆ° {result['count']} å¥—æˆ¿æº:\n"
                                for prop in result["properties"][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                                    function_summary += f"  - {prop['addressLine1']} {prop['addressLine2']}, {prop['bedroomCount']}å®¤{prop['bathroomCount']}å«, {prop['pricePerWeek']}AUD/å‘¨\n"
                        else:
                            function_summary += f"âŒ {func_result['name']}: {result.get('error', 'æŸ¥è¯¢å¤±è´¥')}\n"
                
                # é‡æ–°è°ƒç”¨AIç”ŸæˆåŸºäºå‡½æ•°ç»“æœçš„å›ç­”
                final_messages = messages + [
                    {"role": "assistant", "content": function_summary},
                    {"role": "user", "content": "è¯·æ ¹æ®ä¸Šè¿°æ•°æ®åº“æŸ¥è¯¢ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„ç§Ÿæˆ¿å»ºè®®å’Œæ¨èã€‚"}
                ]
                
                final_response = self.call_qwen_via_dashscope(final_messages, use_functions=False)
                final_answer = final_response["content"]
                
            else:
                # æ™®é€šæ–‡æœ¬å›å¤
                final_answer = response["content"]
            
            # Update history
            self.history.append(("user", query))
            self.history.append(("assistant", final_answer))
            
            return {
                "query": query,
                "vector_context": vector_context,
                "answer": final_answer,
                "function_results": function_results,
                "history": self.history
            }
            
        except Exception as e:
            print(f"Error processing query: {e}")
            print(f"Error type: {type(e)}")
            
            # è¿”å›ä¸€ä¸ªå®‰å…¨çš„é”™è¯¯å“åº”ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            error_message = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•ç®€åŒ–æ‚¨çš„é—®é¢˜æˆ–ç¨åé‡è¯•ã€‚"
            
            # ä»ç„¶æ›´æ–°å†å²è®°å½•ä»¥ä¿æŒå¯¹è¯è¿ç»­æ€§
            self.history.append(("user", query))
            self.history.append(("assistant", error_message))
            
            return {
                "query": query,
                "vector_context": "",
                "answer": error_message,
                "function_results": [],
                "history": self.history,
                "error": str(e)
            }
    
    def clear_history(self):
        """Clear conversation history"""
        self.history = []
    
    def get_history(self):
        """Get conversation history"""
        return self.history


# Global instance
agent = None

def get_agent():
    """Get QrentAgent instance"""
    global agent
    if agent is None:
        agent = QrentAgent()
    return agent


if __name__ == "__main__":
    # Test code
    try:
        agent = QrentAgent()
        result = agent.process_query("I want to find a two-bedroom apartment in Sydney, budget 500 AUD per week")
        print("Query processed successfully!")
        print(f"Answer: {result['answer']}")
    except Exception as e:
        print(f"Error: {e}") 